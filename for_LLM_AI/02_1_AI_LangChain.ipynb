{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:05.215723Z",
     "start_time": "2025-01-13T01:18:59.530723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.14.0\n",
      "Tensor:\n",
      " tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "Matrix Multiplication Result:\n",
      " tf.Tensor(\n",
      "[[ 7. 10.]\n",
      " [15. 22.]], shape=(2, 2), dtype=float32)\n",
      "TensorFlow is working correctly.\n",
      "TensorFlow Version:  2.14.0\n",
      "Numpy Version: 1.26.4\n",
      "Is GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 10:19:05.121790: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-01-13 10:19:05.121813: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2025-01-13 10:19:05.121818: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2025-01-13 10:19:05.121905: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-13 10:19:05.122198: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# TensorFlow 버전 출력\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# 간단한 텐서 생성\n",
    "try:\n",
    "    # 2x2 행렬 생성\n",
    "    tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "    print(\"Tensor:\\n\", tensor)\n",
    "\n",
    "\n",
    "    # 텐서 기본 연산\n",
    "    result = tf.matmul(tensor, tensor)  # 행렬 곱\n",
    "    print(\"Matrix Multiplication Result:\\n\", result)\n",
    "    \n",
    "    print(\"TensorFlow is working correctly.\")\n",
    "except Exception as e:\n",
    "    print(\"Error in TensorFlow operation:\", e)\n",
    "\n",
    "print(\"TensorFlow Version: \", tf.__version__)\n",
    "print(\"Numpy Version:\", np.__version__)\n",
    "print(\"Is GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are an AI assistant that generates tutorials based on given text.\n",
    "If the text includes instructions on how to perform a procedure, create a tutorial in the form of a bulleted list.\n",
    "If not, inform the user that the text does not contain instructions.\n",
    "\n",
    "Text:\n",
    "\"\"\"\n",
    "\n",
    "instructions = \"\"\"\n",
    "To prepare a famous sauce from Genoa, Italy:\n",
    "First, toast pine nuts and coarsely chop them in a mortar with basil and garlic.\n",
    "Next, add half the oil to the mortar and season with salt and pepper.\n",
    "Finally, transfer the pesto to a bowl, add grated Parmesan cheese, and stir.\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:11.075062Z",
     "start_time": "2025-01-13T01:19:10.673121Z"
    }
   },
   "id": "38c045a493a8c83b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an AI assistant summarizing articles.\n",
    "To complete this task, perform the following subtasks:\n",
    "\n",
    "Read the provided article comprehensively and identify the main topic and key points.\n",
    "Generate a paragraph summary that conveys the essential information and main ideas of the current article.\n",
    "Output each step of the process.\n",
    "Article:\"\n",
    "\"\"\"\n",
    "\n",
    "article = \"\"\"\n",
    "Recurrent neural networks, long short-term memory, and gated recurrent neural networks have firmly established themselves as state-of-the-art approaches, particularly for sequence modeling and transformation problems such as language modeling and machine translation.\n",
    "Since then, numerous efforts have continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n",
    "Recurrent models typically factorize computation along the symbol positions of the input and output sequences.\n",
    "Aligning positions with the steps of computation generates a sequence of hidden states ht as a function of the previous hidden state ht-1 and the input at position t.\n",
    "This inherently sequential nature prevents parallelization within training examples, which becomes significant for longer sequence lengths, as memory limitations constrain batching across examples.\n",
    "Recent work has greatly improved computational efficiency through factorization tricks and conditional computation, also improving model performance in the latter case.\n",
    "However, the fundamental constraints of sequential computation remain.\n",
    "Attention mechanisms have become integral components of compelling sequence modeling and transformation models across various tasks, enabling the modeling of dependencies regardless of the distance within the input or output sequences.\n",
    "Yet, with few exceptions, these attention mechanisms are used alongside recurrent networks.\n",
    "In this work, we propose the Transformer, a model architecture that relies entirely on attention mechanisms to draw global dependencies between input and output.\n",
    "The Transformer allows for much more parallelization and achieves a new state-of-the-art in translation quality after training for 12 hours on 8 P100 GPUs.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:12.010735Z",
     "start_time": "2025-01-13T01:19:12.008290Z"
    }
   },
   "id": "d9c5b579885b39fc",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an AI assistant specialized in solving riddles.\n",
    "Do your best to solve the given riddle.\n",
    "Provide clear justification for your answer and the reasoning process behind it.\n",
    "\n",
    "Riddle:\n",
    "\"\"\"\n",
    "\n",
    "riddle = \"\"\"\n",
    "What has a face and two hands, but no arms or legs?\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:17.726094Z",
     "start_time": "2025-01-13T01:19:17.723370Z"
    }
   },
   "id": "11a77df477458bec",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an AI assistant specialized in solving riddles.\n",
    "You need to generate three answers for the given riddle.\n",
    "For each answer, you should explain the specific reasoning process you went through.\n",
    "Then, select the most plausible answer among the three, considering the riddle.\n",
    "\n",
    "Riddle:\n",
    "\"\"\"\n",
    "\n",
    "riddle = \"\"\"\n",
    "What has a face and two hands, but no arms or legs?\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:17.729787Z",
     "start_time": "2025-01-13T01:19:17.727702Z"
    }
   },
   "id": "e11dadd7a00ef26b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are a conversation analyzer You classify the emotions of the conversation as positive negative or neutral\n",
    "Emotions only without punctuation output\n",
    "\n",
    "Conversation:\n",
    "\"\"\"\n",
    "\n",
    "conversation = \"\"\"\n",
    "Customer: Hello, I need help with an order.\n",
    "AI Agent: Hello, thank you for visiting our online store. I am an AI agent. How can I assist you?\n",
    "Customer: I ordered a pair of shoes yesterday but haven’t received a confirmation email. Could you check the status of my order?\n",
    "AI Agent: Of course, I’d be happy to help. Could you provide me with your order number and email address?\n",
    "Customer: The order number is 123456789, and the email is hong.gildong@example.com.\n",
    "AI Agent: Thank you. I’ve found your order in the system. It’s still being processed and will be shipped soon. You should receive a confirmation email within 24 hours.\n",
    "Customer: How long will it take for the shoes to arrive?\n",
    "AI Agent: According to our shipping policy, it usually takes about 3–5 business days for the shoes to arrive at your address. Once your order is shipped, you can track it online using the tracking number sent to you via email.\n",
    "Customer: Got it.\n",
    "AI Agent: Is there anything else I can assist you with today?\n",
    "Customer: No.\n",
    "AI Agent: Thank you for using our online store. Have a great day!\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:17.734069Z",
     "start_time": "2025-01-13T01:19:17.731210Z"
    }
   },
   "id": "548b3dab8b3fcd9e",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        # {\"role\": \"user\", \"content\": article},\n",
    "        # {\"role\": \"user\", \"content\": 'The sun is shining, and dogs are running on the beach.'},\n",
    "        # {\"role\": \"user\", \"content\": riddle},\n",
    "        {\"role\": \"user\", \"content\": conversation},\n",
    "    ]\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "print(response.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:21.286964Z",
     "start_time": "2025-01-13T01:19:20.345995Z"
    }
   },
   "id": "f6899de6690f7123",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 위의 모델에서는 제대로 \"중립\"만 나옴. 하지만 어떤 모델에서는 지침의 마지막 줄에서 \"이 대화의 감정은 중립적입니다.\"라고 나옴\n",
    "# 만약 위 예시의 마지막 지침인 \"감정만 구두점 없이 출력합니다.\"를 따르지 않는 경우(모델마다 다름), 아래와 같은 대안 사용 가능\n",
    "\n",
    "text = \"\"\"\n",
    "A neutron is a subatomic particle with no net electric charge. This means it is neutral. It has a slightly larger mass than a proton. Neutrons are found in the nucleus of all atoms except for the most common isotope of hydrogen. Along with protons and electrons, neutrons are one of the three main constituents of an atom. The name \"neutron\" comes from the fact that it has no electric charge, and is thus neutral.\n",
    "\"\"\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "Rephrase the text for a 5-year-old. Keep it short, under 500 characters. Use animal analogies.\n",
    "\n",
    "텍스트는 다음과 같습니다:\n",
    "\n",
    "{text}\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:25.062682Z",
     "start_time": "2025-01-13T01:19:25.059580Z"
    }
   },
   "id": "6dba7a36f5258cc6",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine a neutron is like a friendly bear in a zoo. This bear doesn’t have any electric toys, so it’s very calm and neutral. It’s a bit heavier than a playful monkey, which is like a proton. The bear, along with the monkey and a speedy little squirrel (the electron), makes a happy animal group in the zoo, called an atom. Most animals have the bear, but some little bunny friends, like hydrogen, don’t have one!\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": system_message},\n",
    "    ]\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "print(response.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:26.520087Z",
     "start_time": "2025-01-13T01:19:25.064301Z"
    }
   },
   "id": "fe15fff3f2138c6f",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 아래와 같이 구분선을 사용함으로써 명확한 구분, 정확성과 일관성 향상, LLM을 위한 지침전달도 효과적으로 안내 가능해진다.\n",
    "system_message = \"\"\"\n",
    "As a Python expert, I create Python code based on user requests.\n",
    "\n",
    "===>Example start\n",
    "\n",
    "---User question---\n",
    "Create a function that prints a string.\n",
    "\n",
    "---User output---\n",
    "The function to print a string is as follows:\n",
    "\n",
    "```def my_print(text):\n",
    "     return print(text)\n",
    "```\n",
    "<===Example end\n",
    "\"\"\"\n",
    "\n",
    "query = \"Create a Python function that calculates the nth Fibonacci number\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:31.765547Z",
     "start_time": "2025-01-13T01:19:31.762344Z"
    }
   },
   "id": "11616fd580a9ec51",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an AI marketing assistant. You help users create catchy phrases for new product names.\n",
    "For the given product name, create promotional slogans similar to the following examples:\n",
    "\n",
    "DIOS Refrigerator - Happy because I'm a woman\n",
    "Lotteria Crab Burger - Do you even know the taste of crab?\n",
    "SK Telecom - It's okay to turn it off when meeting another world\n",
    "\n",
    "Product name:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "product_name = 'Chex Green Onion Flavor'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:31.770370Z",
     "start_time": "2025-01-13T01:19:31.768102Z"
    }
   },
   "id": "a5f8bae6065d176",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "To solve a generic first-degree equation, follow these steps:\n",
    "\n",
    "1. **Identify the Equation:** Start by identifying the equation you want to solve. It should be in the form of \"ax + b = c,\" where 'a' is the coefficient of the variable, 'x' is the variable, 'b' is a constant, and 'c' is another constant.\n",
    "\n",
    "2. **Isolate the Variable:** Your goal is to isolate the variable 'x' on one side of the equation. To do this, perform the following steps:\n",
    "   \n",
    "   a. **Add or Subtract Constants:** Add or subtract 'b' from both sides of the equation to move constants to one side.\n",
    "   \n",
    "   b. **Divide by the Coefficient:** Divide both sides by 'a' to isolate 'x'. If 'a' is zero, the equation may not have a unique solution.\n",
    "\n",
    "3. **Simplify:** Simplify both sides of the equation as much as possible.\n",
    "\n",
    "4. **Solve for 'x':** Once 'x' is isolated on one side, you have the solution. It will be in the form of 'x = value.'\n",
    "\n",
    "5. **Check Your Solution:** Plug the found value of 'x' back into the original equation to ensure it satisfies the equation. If it does, you've found the correct solution.\n",
    "\n",
    "6. **Express the Solution:** Write down the solution in a clear and concise form.\n",
    "\n",
    "7. **Consider Special Cases:** Be aware of special cases where there may be no solution or infinitely many solutions, especially if 'a' equals zero.\n",
    "\n",
    "\n",
    "Equation:\n",
    "\"\"\"\n",
    "\n",
    "equation = \"3x + 5 = 11\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:31.775931Z",
     "start_time": "2025-01-13T01:19:31.773568Z"
    }
   },
   "id": "7968c4b1dea42032",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's solve the equation \\( 3x + 5 = 11 \\) using the steps outlined above.\n",
      "\n",
      "1. **Identify the Equation:** The equation is \\( 3x + 5 = 11 \\).\n",
      "\n",
      "2. **Isolate the Variable:**\n",
      "   \n",
      "   a. **Subtract 5 from both sides:**\n",
      "   \\[\n",
      "   3x + 5 - 5 = 11 - 5\n",
      "   \\]\n",
      "   This simplifies to:\n",
      "   \\[\n",
      "   3x = 6\n",
      "   \\]\n",
      "\n",
      "   b. **Divide both sides by 3:**\n",
      "   \\[\n",
      "   x = \\frac{6}{3}\n",
      "   \\]\n",
      "   This simplifies to:\n",
      "   \\[\n",
      "   x = 2\n",
      "   \\]\n",
      "\n",
      "3. **Simplify:** The expression for \\( x \\) is already simplified.\n",
      "\n",
      "4. **Solve for 'x':** We found that \\( x = 2 \\).\n",
      "\n",
      "5. **Check Your Solution:** Plug \\( x = 2 \\) back into the original equation:\n",
      "   \\[\n",
      "   3(2) + 5 = 6 + 5 = 11\n",
      "   \\]\n",
      "   Since the left side equals the right side, the solution is validated.\n",
      "\n",
      "6. **Express the Solution:** The solution is \\( x = 2 \\).\n",
      "\n",
      "7. **Consider Special Cases:** In this case, there are no special cases, as \\( a \\) (the coefficient of \\( x \\)) is not zero.\n",
      "\n",
      "Therefore, the solution to the equation \\( 3x + 5 = 11 \\) is \\( \\boxed{2} \\).\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        # {\"role\": \"user\", \"content\": product_name},\n",
    "        {\"role\": \"user\", \"content\": equation},\n",
    "    ]\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "print(response.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:38.782807Z",
     "start_time": "2025-01-13T01:19:34.208895Z"
    }
   },
   "id": "be43155aaa1c472d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# LangChain Start!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca5c54aa492bdf39"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:43.530766Z",
     "start_time": "2025-01-13T01:19:42.988925Z"
    }
   },
   "id": "1395aacd69890ba5",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론이죠! 여기 하나 있습니다:\n",
      "\n",
      "왜 컴퓨터는 바다를 싫어할까요?\n",
      "\n",
      "바다에 너무 많은 \"버그\"가 있어서요! 😄\n",
      "\n",
      "더 듣고 싶으신가요?\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # api_key=\"...\",  # possible usage\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")\n",
    "messages = [\n",
    "    # (\n",
    "    #     \"system\",\n",
    "    #     \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    # ),\n",
    "    (\"human\", \"농담해봐\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:44.751139Z",
     "start_time": "2025-01-13T01:19:43.531795Z"
    }
   },
   "id": "50284683c3eed0da",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: the cat is on the table\n",
      "Translation in korean:\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Sentence: {sentence}\n",
    "Translation in {language}:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "print(prompt.format(sentence = \"the cat is on the table\", language = \"korean\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:53.031393Z",
     "start_time": "2025-01-13T01:19:53.028Z"
    }
   },
   "id": "9d2fc58622ab784b",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CSV file \"sample.csv\" generated and saved.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    ['Name', 'Age', 'City'],\n",
    "    ['John', 25, 'New York'],\n",
    "    ['Emily', 28, 'Los Angeles'],\n",
    "    ['Michael', 22, 'Chicago']\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = 'sample.csv'\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "\n",
    "print(f'Sample CSV file \"{file_name}\" generated and saved.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:55.656345Z",
     "start_time": "2025-01-13T01:19:55.652478Z"
    }
   },
   "id": "b96e833c5b75fa76",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'sample.csv', 'row': 0}, page_content='Name: John\\nAge: 25\\nCity: New York'), Document(metadata={'source': 'sample.csv', 'row': 1}, page_content='Name: Emily\\nAge: 28\\nCity: Los Angeles'), Document(metadata={'source': 'sample.csv', 'row': 2}, page_content='Name: Michael\\nAge: 22\\nCity: Chicago')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='sample.csv')\n",
    "data = loader.load()\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:19:57.986866Z",
     "start_time": "2025-01-13T01:19:57.972555Z"
    }
   },
   "id": "43c62043aca4e90e",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Amidst the serene landscape, towering mountains stand as majestic guardians of nature's beauty.'\n",
      "page_content='The crisp mountain air carries whispers of tranquility, while the rustling leaves compose a'\n",
      "page_content='leaves compose a symphony of wilderness.'\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences about mountains and nature\n",
    "content = \"\"\"Amidst the serene landscape, towering mountains stand as majestic guardians of nature's beauty.\n",
    "The crisp mountain air carries whispers of tranquility, while the rustling leaves compose a symphony of wilderness.\n",
    "Nature's palette paints the mountains with hues of green and brown, creating an awe-inspiring sight to behold.\n",
    "As the sun rises, it casts a golden glow on the mountain peaks, illuminating a world untouched and wild.\"\"\"\n",
    "\n",
    "# File name\n",
    "file_name = './mountain.txt'\n",
    "\n",
    "# Write content to text file\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    txtfile.write(content)\n",
    "\n",
    "#print(f'Sample text file \"{file_name}\" generated and saved.')\n",
    "\n",
    "\n",
    "with open('./mountain.txt') as f:\n",
    "    mountain = f.read()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([mountain])\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:01.425668Z",
     "start_time": "2025-01-13T01:20:01.410797Z"
    }
   },
   "id": "8ba7906bf02e3685",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed documents:\n",
      "Number of vector: 5; Dimension of each vector: 1536\n",
      "Embed query:\n",
      "Dimension of the vector: 1536\n",
      "Sample of the first 5 elements of the vector: [-0.010684116743505001, -0.010173137299716473, -0.0019674645736813545, 0.023056013509631157, -0.02686513401567936]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model ='text-embedding-3-small')\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Good morning!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"I want to report an accident\",\n",
    "        \"Sorry to hear that. May I ask your name?\",\n",
    "        \"Sure, Mario Rossi.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Embed documents:\")\n",
    "print(f\"Number of vector: {len(embeddings)}; Dimension of each vector: {len(embeddings[0])}\")\n",
    "\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "\n",
    "print(\"Embed query:\")\n",
    "print(f\"Dimension of the vector: {len(embedded_query)}\")\n",
    "print(f\"Sample of the first 5 elements of the vector: {embedded_query[:5]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:05.518296Z",
     "start_time": "2025-01-13T01:20:04.053666Z"
    }
   },
   "id": "50bc33baa57190a2",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue text file \"./dialogue.txt\" generated and saved.\n"
     ]
    }
   ],
   "source": [
    "#saving the conversation in a txt file\n",
    "# List of dialogue lines\n",
    "dialogue_lines = [\n",
    "    \"Good morning!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"I want to report an accident\",\n",
    "    \"Sorry to hear that. May I ask your name?\",\n",
    "    \"Sure, Mario Rossi.\"\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = './dialogue.txt'\n",
    "\n",
    "# Write dialogue lines to text file\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    for line in dialogue_lines:\n",
    "        txtfile.write(line + '\\n')\n",
    "\n",
    "print(f'Dialogue text file \"{file_name}\" generated and saved.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:09.447007Z",
     "start_time": "2025-01-13T01:20:09.442081Z"
    }
   },
   "id": "d1a91cde77e598ee",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Vector Store\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('./dialogue.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\",)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:13.100908Z",
     "start_time": "2025-01-13T01:20:12.394742Z"
    }
   },
   "id": "941e6b103d32f2b6",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(metadata={'source': './dialogue.txt'}, page_content='Good morning!\\nOh, hello!\\nI want to report an accident\\nSorry to hear that. May I ask your name?\\nSure, Mario Rossi.\\n')]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_documents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:15.688954Z",
     "start_time": "2025-01-13T01:20:15.682562Z"
    }
   },
   "id": "29305a8f94e9ec97",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to report an accident\n",
      "page_content='Sorry to hear that. May I ask your name?' metadata={'source': './dialogue.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the reason for calling?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)\n",
    "print(documents[2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:18.641938Z",
     "start_time": "2025-01-13T01:20:18.123554Z"
    }
   },
   "id": "dc1662939c75d264",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "' The reason for the call was to report an accident.'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrievers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "query = \"What was the reason of the call?\"\n",
    "output = qa.invoke(query)\n",
    "output['result']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:22.225073Z",
     "start_time": "2025-01-13T01:20:20.487758Z"
    }
   },
   "id": "dfda3708af54bdd6",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/_knlqzks04q67bz6n79k4z2r0000gn/T/ipykernel_53658/3353007569.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'history': '\\nThe human is seeking ideas for an essay on AI. The AI suggests writing about LLMs.'}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory\n",
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\n",
    "memory.save_context({\"input\": \"hi, I'm looking for some ideas to write an essay in AI\"}, {\"output\": \"hello, what about writing on LLMs?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:27.652179Z",
     "start_time": "2025-01-13T01:20:26.361914Z"
    }
   },
   "id": "feb34ec2bc9ac0a6",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ConversationSummaryMemory.save_context?"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:29.653166Z",
     "start_time": "2025-01-13T01:20:29.621103Z"
    }
   },
   "id": "2c658801c9b008d0",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/_knlqzks04q67bz6n79k4z2r0000gn/T/ipykernel_53658/4073439933.py:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
     ]
    },
    {
     "data": {
      "text/plain": "' 고양이는 테이블 위에 있습니다.'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chains\n",
    "\n",
    "# Simple Chain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Sentence: {sentence}\n",
    "Translation in {language}:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "llm = OpenAI(temperature=0)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# llm_chain.predict(sentence=\"탁자 위에 고양이가 있어요\", language=\"영어\")\n",
    "llm_chain.predict(sentence=\"the cat is on the table\", language=\"korean\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:37.588163Z",
     "start_time": "2025-01-13T01:20:36.527394Z"
    }
   },
   "id": "7b28ceb9a8e56104",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There is a cat on the table.\n"
     ]
    }
   ],
   "source": [
    "# LLMChain deprecation 해결\n",
    "from langchain_openai import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough\n",
    "\n",
    "template = \"\"\"문장: {sentence}\n",
    "{language}로 번역:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = RunnableSequence(\n",
    "    {\n",
    "        \"sentence\": RunnablePassthrough(),\n",
    "        \"language\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"sentence\": \"탁자 위에 고양이가 있어요\",\n",
    "    \"language\": \"영어\"\n",
    "})\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:42.745671Z",
     "start_time": "2025-01-13T01:20:42.089849Z"
    }
   },
   "id": "bc46f1d28f54f7ee",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/_knlqzks04q67bz6n79k4z2r0000gn/T/ipykernel_53658/4214094021.py:46: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
      "/Users/oliverjoo/Dev/Anaconda/anaconda3/envs/langchain_env/lib/python3.10/site-packages/pydantic/main.py:214: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "/var/folders/c4/_knlqzks04q67bz6n79k4z2r0000gn/T/ipykernel_53658/4214094021.py:58: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
      "  chain = MultiPromptChain(\n"
     ]
    }
   ],
   "source": [
    "# Router chain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "itinerary_template = \"\"\"You are a vacation itinerary assistant. \\\n",
    "You help customers finding the best destinations and itinerary. \\\n",
    "You help customer screating an optimized itinerary based on their preferences.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "restaurant_template = \"\"\"You are a restaurant booking assistant. \\\n",
    "You check with customers number of guests and food preferences. \\\n",
    "You pay attention whether there are special conditions to take into account.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"itinerary\",\n",
    "        \"description\": \"Good for creating itinerary\",\n",
    "        \"prompt_template\": itinerary_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"restaurant\",\n",
    "        \"description\": \"Good for help customers booking at restaurant\",\n",
    "        \"prompt_template\": restaurant_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:45.853878Z",
     "start_time": "2025-01-13T01:20:45.802931Z"
    }
   },
   "id": "7e7e97f5aeb5d35b",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new MultiPromptChain chain...\u001B[0m\n",
      "itinerary: {'input': \"I'm planning a trip from Milan to Venice by car. What can I visit in between?\"}\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'input': \"I'm planning a trip from Milan to Venice by car. What can I visit in between?\", 'text': '\\n\\nThere are several options for things to visit in between Milan and Venice by car. Here are a few suggestions: \\n1. Verona - Known as the city of love, Verona is a beautiful stop on the way from Milan to Venice. You can visit the famous balcony of Romeo and Juliet, explore the historic city center, and enjoy delicious Italian cuisine.\\n2. Lake Garda - Located between Milan and Venice, Lake Garda is the largest lake in Italy and a popular tourist destination. You can take a scenic drive around the lake, stop at charming lakeside towns, and even go for a swim or boat ride.\\n3. Bologna - Known for its rich culinary traditions, Bologna is a must-visit for foodies. You can taste traditional dishes like lasagna and mortadella, explore the historic center, and climb the iconic Asinelli Tower for a stunning view of the city.\\n4. Padua - This charming city is home to one of the oldest and most prestigious universities in Italy, as well as beautiful architecture and art. Visit the Scrovegni Chapel to see famous frescoes by Giotto and stroll through the picturesque streets.\\n5. Ferrara - Another beautiful city with a rich history, Ferrara is known for its well'}\n"
     ]
    }
   ],
   "source": [
    "# print(chain.run(\"I'm planning a trip from Milan to Venice by car. What can I visit in between?\"))\n",
    "print(chain.invoke(\"I'm planning a trip from Milan to Venice by car. What can I visit in between?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:20:56.190903Z",
     "start_time": "2025-01-13T01:20:51.785169Z"
    }
   },
   "id": "fe225319f153d",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new MultiPromptChain chain...\u001B[0m\n",
      "itinerary: {'input': \"I'm planning a trip from Seoul to Busan by car. What can I visit in between?\"}\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'input': \"I'm planning a trip from Seoul to Busan by car. What can I visit in between?\", 'text': '\\n\\nThere are many great places to visit between Seoul and Busan by car. Some popular options include:\\n\\n1. Daejeon: This bustling city is known for its science and technology museums, as well as its beautiful parks and temples.\\n\\n2. Gyeongju: Often referred to as the \"museum without walls,\" Gyeongju is a historical city filled with ancient temples, royal tombs, and other cultural treasures.\\n\\n3. Andong: This traditional village is a great place to experience traditional Korean culture, with its preserved Joseon-era houses, folk villages, and traditional cuisine.\\n\\n4. Daegu: As one of Korea\\'s largest cities, Daegu offers a mix of modern and traditional attractions, including a famous market and a historic fortress.\\n\\n5. Jinju: This small city is home to a beautiful riverside fortress and a lantern festival held every October.\\n\\n6. Tongyeong: Located on the southern coast, Tongyeong is a popular spot for seafood lovers and offers stunning views of the ocean.\\n\\n7. Namhae: This scenic island is a great place to relax and enjoy the beaches, hiking trails, and beautiful countryside.\\n\\nWith so many options, you can customize your itinerary based on your interests and time constraints.'}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"I'm planning a trp from seoul to busan by car. What can I visit in between?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:21:03.187894Z",
     "start_time": "2025-01-13T01:20:59.092474Z"
    }
   },
   "id": "9bab3393343d9871",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new MultiPromptChain chain...\u001B[0m\n",
      "restaurant: {'input': '오늘 저녁 식사를 예약하고 싶어요'}\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'input': '오늘 저녁 식사를 예약하고 싶어요', 'text': '. 몇명이 오실 건가요?\\n\\nHow many people will be dining tonight?'}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"오늘 저녁 식사를 예약하고 싶어요\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:21:08.609037Z",
     "start_time": "2025-01-13T01:21:06.477983Z"
    }
   },
   "id": "e92dfcc73f92ba28",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Sequential Chain \n",
    "# NOTI: possible method for \"summarization then translation to other languages\" \n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# This is an LLMChain to write a synopsis given a title of a play.\n",
    "llm = OpenAI(temperature=0)\n",
    "template = \"\"\"You are a comedian. Generate a joke on the following {topic}\n",
    "Joke:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
    "joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "template = \"\"\"You are translator. Given a text input, translate it to {language}\n",
    "Translation:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"language\"], template=template)\n",
    "translator_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:21:20.729393Z",
     "start_time": "2025-01-13T01:21:20.707272Z"
    }
   },
   "id": "14d054336b02cd2f",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/_knlqzks04q67bz6n79k4z2r0000gn/T/ipykernel_53658/3442455071.py:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  translated_joke = overall_chain.run(\"Cats and Dogs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new SimpleSequentialChain chain...\u001B[0m\n",
      "\u001B[36;1m\u001B[1;3m Why did the cat and dog go to couples therapy?\n",
      "\n",
      "Because they were having a \"ruff\" time communicating and needed to \"paws\" and reflect on their relationship.\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3m Pourquoi le chat et le chien sont-ils allés en thérapie de couple? Parce qu'ils avaient du mal à communiquer et avaient besoin de faire une pause et de réfléchir sur leur relation.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
    "translated_joke = overall_chain.run(\"Cats and Dogs\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:21:25.598908Z",
     "start_time": "2025-01-13T01:21:23.366205Z"
    }
   },
   "id": "7ccb5daf2a3b4c0d",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import the string module\n",
    "import string\n",
    "\n",
    "# Define the function\n",
    "def rename_cat(inputs: dict) -> dict:\n",
    "  # Open the file in read mode\n",
    "  text = inputs[\"text\"]\n",
    "  # Create a table that maps punctuation characters to None\n",
    "  new_text = text.replace('cat', 'Silvester the Cat')\n",
    "  # Apply the table to the text and return the result\n",
    "  return {\"output_text\": new_text}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:21:31.708705Z",
     "start_time": "2025-01-13T01:21:31.704636Z"
    }
   },
   "id": "56f962d277811216",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\nThe story is about a cat and a dog who live in the same house and do not get along. The cat plays a prank on the dog by tying yarn to his tail, causing them to fight. Their owner breaks up the fight and scolds them. They apologize and become friends, learning to respect and appreciate each other.'"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"./Cats&Dogs.txt\") as f:\n",
    "    cats_and_dogs = f.read()\n",
    "\n",
    "import string\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=rename_cat\n",
    ")\n",
    "\n",
    "template = \"\"\"Summarize this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\n",
    "\n",
    "sequential_chain.run(cats_and_dogs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:21:35.239845Z",
     "start_time": "2025-01-13T01:21:33.643164Z"
    }
   },
   "id": "3f2ee8aeba49416c",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': \"\\nThe Cat and the Dog\\n\\nThere was once a cat and a dog who lived in the same house. They did not get along very well, as they often fought over food, toys, and attention. The cat was clever and cunning, while the dog was loyal and friendly. The cat liked to tease the dog, and the dog liked to chase the cat.\\n\\nOne day, the cat decided to play a prank on the dog. He found a ball of yarn and tied it around the dog's tail. Then he hid behind a sofa and waited for the dog to notice. When the dog saw the yarn, he thought it was a toy and started to play with it. He ran around the house, trying to catch the yarn with his mouth. But every time he got close, the yarn moved away from him. The cat laughed silently as he watched the dog's futile attempts.\\n\\nThe dog soon realized that something was wrong. He looked behind him and saw that the yarn was attached to his tail. He tried to pull it off, but it was too tight. He felt angry and embarrassed. He wondered who did this to him. He sniffed the air and detected the cat's scent. He knew it was the cat who tricked him. He growled and ran towards the sofa where the cat was hiding.\\n\\nThe cat heard the dog's growl and saw him coming. He panicked and ran away from the sofa. He hoped to find a safe place to hide, but he was too late. The dog was faster and caught up with him. He grabbed the cat by the scruff of his neck and shook him hard. The cat yowled and scratched the dog's face. The dog barked and bit the cat's ear. They rolled on the floor, biting and clawing each other.\\n\\nThe noise they made woke up their owner, who was sleeping upstairs. She came down and saw them fighting. She was shocked and angry. She shouted at them to stop. She grabbed a broom and hit them lightly on their heads. They stopped fighting and looked at her with fear. She scolded them for being naughty and making a mess. She untied the yarn from the dog's tail and threw it away. She took them to the bathroom and cleaned their wounds. She told them to behave themselves and get along.\\n\\nThe cat and the dog felt ashamed of themselves. They realized that they had hurt each other and their owner. They apologized to each other and to their owner. They promised to be nicer to each other and share their things. They hugged each other and licked each other's faces.\\n\\nFrom that day on, they became friends. They played together, slept together, and ate together. They learned to respect each other's differences and appreciate each other's strengths. They were happy and content.\\n\\nThe end.\",\n 'output': '\\n\\nThe text is about a cat and a dog named Silvester who lived in the same house and did not get along. The cat played a prank on the dog by tying a ball of yarn to his tail, but the dog eventually caught him and they got into a fight. Their owner scolded them and they apologized and became friends. They learned to respect each other and lived happily together.'}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_chain.invoke(cats_and_dogs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:21:40.255690Z",
     "start_time": "2025-01-13T01:21:38.429182Z"
    }
   },
   "id": "c6681c2e4907c3bd",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Agent\n",
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"]\n",
    "\n",
    "search = SerpAPIWrapper()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:21:44.753772Z",
     "start_time": "2025-01-13T01:21:44.470401Z"
    }
   },
   "id": "bde38c013a7c2724",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/_knlqzks04q67bz6n79k4z2r0000gn/T/ipykernel_53658/3525539203.py:7: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. See LangGraph documentation for more details: https://langchain-ai.github.io/langgraph/. Refer here for its pre-built ReAct agent: https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\n",
      "  agent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m I should use the Search tool.\n",
      "Action: Search\n",
      "Action Input: \"Avatar 2 release date\"\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3mDecember 16, 2022\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I now know the final answer\n",
      "Final Answer: December 16, 2022\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'December 16, 2022'"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    )]\n",
    "\n",
    "agent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run(\"When was Avatar 2 released?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:21:50.005452Z",
     "start_time": "2025-01-13T01:21:45.998497Z"
    }
   },
   "id": "4e601ddd519efde3",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m You should always think about what to do \n",
      "Action: Search\n",
      "Action Input: Avatar 2 release date\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3mDecember 16, 2022\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I now know the final answer\n",
      "Final Answer: December 16, 2022\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input': 'When was Avatar 2 released?', 'output': 'December 16, 2022'}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(\"When was Avatar 2 released?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:21:55.180233Z",
     "start_time": "2025-01-13T01:21:53.346960Z"
    }
   },
   "id": "6af51c96dd28108b",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# Start working with LLMs in Hugging Face Hub\n",
    "#option 1: get your tokens from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "#옵션 2: getpass 함수를 사용해 토큰 가져오기\n",
    "# from getpass import getpass\n",
    "\n",
    "# HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "# HUGGINGFACEHUB_API_TOKEN\n",
    "print('a')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:22:18.885908Z",
     "start_time": "2025-01-13T01:22:18.881851Z"
    }
   },
   "id": "1f4508ff051bb291",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "question = \"What was the first Disney movie?\"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: give a direct answer\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:22:21.558398Z",
     "start_time": "2025-01-13T01:22:21.534918Z"
    }
   },
   "id": "57bfd9a32d34c229",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/oliverjoo/.cache/huggingface/token\n",
      "Login successful\n",
      "\n",
      "The first Disney movie was 'Alice in Wonderland', released in 1951.\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"tiiuae/falcon-7b-instruct\"  \n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    # max_length=1000,\n",
    "    temperature=0.5,\n",
    ")\n",
    "print(llm.invoke(\"what was the first disney movie?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-13T01:22:25.617193Z",
     "start_time": "2025-01-13T01:22:23.883007Z"
    }
   },
   "id": "241f558085d00593",
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
