{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-03T08:32:50.416970Z",
     "start_time": "2025-02-03T08:32:50.144163Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def transcribe(audio_filepath, **kwargs) -> str:\n",
    "    if 'response_format' not in kwargs:\n",
    "        kwargs['response_format'] = 'text'  # response_format에 따라 반환 형식이 달라져서 오류가 발생하지 않도록 기본값인 \"json\" 대신 \"text\"를 사용\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        file=open(audio_filepath, \"rb\"),\n",
    "        model=\"whisper-1\",\n",
    "        **kwargs\n",
    "    )\n",
    "    return transcript"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T08:32:51.937013Z",
     "start_time": "2025-02-03T08:32:51.933255Z"
    }
   },
   "id": "55b499fabd31f05",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transcribe Audio File"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6707874725f247c2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사람들이 좋아할 만한 것을 만들어보아요.\n"
     ]
    }
   ],
   "source": [
    "audio_file_path = \"speech.mp3\"\n",
    "print(transcribe(audio_file_path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T08:14:37.309522Z",
     "start_time": "2025-02-03T08:14:35.744180Z"
    }
   },
   "id": "c94f8e15d4785154",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For optimal performance, use pytubefix version '8.12.1'. Version '8.1.1' would encounter errors\n",
    "from pytubefix import YouTube\n",
    "     \n",
    "video_url = 'https://www.youtube.com/watch?v=TLRTml68cYM'\n",
    "\n",
    "video = YouTube(video_url).streams.filter(only_audio=True).first().download()  # https://stackoverflow.com/a/50041191"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T08:38:14.813232Z",
     "start_time": "2025-02-03T08:38:11.021968Z"
    }
   },
   "id": "9d33352f4714c10f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"AI just got some serious shakeup and it's all because of DeepSync. This new AI model isn't just another release, it's a game changer. It's got OpenAI scrambling, NVIDIA stop taking hits, and the AI industry rethinking everything. And you might ask why? This is because it delivers top-tier AI performance for a fraction of the usual cost and it was built in the last two months. If AI companies no longer need billion-dollar budgets to train their models, what happens next? Let's break it down. So, what is DeepSync R1? In simple terms, it's an open-source AI model out of China that's punching way above its weight. In fact, it's on par with OpenAI's $200 per month O1 model at a lot of things like coding, research, and even maths. And it's free. You can even host it yourself if you don't trust them. But what makes it really special is the fact that it was trained for just $5.5 million, roughly $6 million. In the AI world right now, that's like getting a brand new Tesla for the price of a used Honda Civic. Compare that to OpenAI's GPT-4, which reportedly costs upwards of $200 million to train. And you start to see why this is making waves. But here's where everything gets crazier. DeepSync R1 wasn't trained on NVIDIA's most powerful GPUs. Thanks to US sanctions, China doesn't have access to NVIDIA's cutting-edge AI chips, like the H100. Instead, they have to make do with the H800, which are essentially NVIDIA's nerfed versions for the Chinese market. And yet, despite their hardware disadvantage, DeepSync R1 performs on par with OpenAI's O1 model, and even clawed 3.5% from Anthropic. Now let's talk about why this matters. First off, NVIDIA's stock took a hit. You may be thinking why? Because if AI companies can train high-performing models using cheaper and less powerful hardware, it is assumed that the demand for NVIDIA's top-tier AI chips could shrink. And considering NVIDIA's valuation has been largely propped by the AI boom, anything that threatens that demand sends investors into a panic. And it's not just NVIDIA feeling the heat. If AI models can now be trained cheaper and more efficiently, companies like OpenAI, Google, and Microsoft have to rethink their strategies. DeepSync's open-source decision has already forced OpenAI into action. I mean, some alt-man announced that the O1 mini-model, which was previously locked behind a paywall, will now be free. Clearly, a reaction to DeepSync making cutting-edge AI accessible to everyone. OpenAI realizes that if people have to access a powerful model like DeepSync R1, they need to match that move or risk losing their dominance. DeepSync R1 is already proving to be a serious competitor in the AI space. On the App Store right now, it has overtaken ChatGPT to become one of the most-downloaded AI apps. This isn't just a niche product for AI enthusiasts. Regular users are flocking to it, showing that there's a real demand for an open-source alternative to OpenAI's world-guarding approach. But here's where things get really interesting. The AI arms race just entered a new phase, and one where open-source models are becoming serious contenders. And that means AI development could start moving at an even faster pace. Think about it. When OpenAI keeps ChatGPT 4 under lock and key, only a handful of companies get to improve it. But when a model like DeepSync R1 is released as open-source, thousands of developers around the world can refine it, optimize it, and push the technology forward in ways a single company never could. This could be the beginning of the end for closed-source AI dominance. I know there's glamour from Meta, but it's good to see that there are more options out there doing the same thing. One of the biggest advantages of open-source AI models like DeepSync R1 is accessibility. Unlike proprietary models such as OpenAI's GPT-4, which requires massive funding and closed-door collaborations, open-source models lower the barrier to entry. Take Meta's LLAMA models for example. These pre-trained models allow developers, researchers, and startups to build upon them rather than starting from scratch, which would be nearly impossible for individuals due to their high computational and financial costs. Additionally, open-source approach encourages transparency, allowing researchers to scrutinize the model, detect biases, and improve the safety measures. With more eyes on the code and training methodologies, AI development becomes more accountable and aligned with real-world applications. This kind of collaborative progress is something that closed-source models can't replicate at the same scale. While DeepSync R1 is the model everyone is talking about right now, it's actually DeepSync v3 running underneath. And this is how it works. I mean, this is a simplified version of how it works, but it's much more complex in real life. DeepSync uses an approach called mixture of experts. Instead of treating every query as a general problem, it delegates tasks to specialized submodels trained for specific functions. So when you ask DeepSync a question, it figures which experts should handle it and routes that question or request accordingly. This means that if you're running an 8 billion parameter model, it doesn't need to load all the 8 billion parameters into memory, just the portion relevant to your query. That dramatically reduces the memory usage, speeds up the response time and lowers the power consumption. It's a smarter and more efficient way to handle AI processing. I've personally spent the past three days extensively testing DeepSync R1 and I have to say it's an absolute game changer. It showcases an advanced level of reasoning, handling complex logical problems with a precision I've rarely seen in open source AI. It has the ability to perform chain of thought reasoning, so instead of spitting out an answer immediately, it works through the problem step by step, evaluating and even correcting itself in real time. You'll see the whole thought process as it runs. If DeepSync can achieve this level of reasoning with lower cost training, it suggests that AI models no longer need billion dollar budgets. That completely shifts the AI race, making high-end capabilities accessible to more players and potentially threatening the current AI giants who have relied on their deep pockets to maintain dominance. Some analysts are questioning whether DeepSync R1 was really trained on as few GPUs as claimed. Others speculate that China might have secretly used more powerful hardware or borrowed techniques from existing western models. I even saw screenshots of DeepSync responding as ChatGPT on the web and that allegedly means it was trained on ChatGPT's data. I can't really verify how true that is but even if there's some truth to that, it doesn't change the bigger picture. AI is getting cheaper and more accessible. By the way, if you're skeptical about data privacy and worried about DeepSync sending your queries to Chinese servers, let's be real, who says America isn't doing that right now? Allegedly. The good news is, you don't have to rely on cloud-based services to use DeepSync. You can run it locally, I mean it's open source. For instance, I'm running the 8 billion parameter model on my MacBook Pro, the M1 chip and it's between 4.5 to 5 gigs. Meanwhile, we've got the 14 billion parameter model running on our in-house server downstairs and that's about 9 gigs. That means all processing happens on device and no data is sent over the internet. If you want a more user-friendly way to run this, tools like OpenWebUI or LM Studio provide a great interface for running AI models locally. So no subscriptions, no data privacy concerns, just pure local AI power. But here's something else to consider. If training AI becomes more accessible, it might actually create even more demand for AI infrastructure. And the lower barrier to entry means more startups, research labs and even individuals could jump into the AI game. And inference, which is the process of using AI models, still requires massive amounts of computing power and that's where NVIDIA still has an edge. Ironically, this could end up benefiting NVIDIA in the long run. Who knows? The initial stock dip could be just a temporary market overreaction before the demand for AI infrastructure explodes again because we now know it can be cheaper to run these companies, right? So what do you think? Is this the beginning of open-source AI revolution or just another hype cycle? Drop your thoughts in the comment section below. And if you enjoyed this breakdown, do not forget to subscribe for more insights on how AI is shaping the future. Until next time, stay curious and stay ahead of the curve. Catch you in the next video. Cuidate!\\n\""
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe(video)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T08:38:36.335550Z",
     "start_time": "2025-02-03T08:38:14.815332Z"
    }
   },
   "id": "3f87a4bcb40defcf",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:00,000 --> 00:00:03,920\n",
      "AI just got some serious shake-up, and it's all because of DeepSeek.\n",
      "\n",
      "2\n",
      "00:00:03,920 --> 00:00:09,440\n",
      "This new AI model isn't just another release, it's a game-changer. It's got OpenAI scrambling,\n",
      "\n",
      "3\n",
      "00:00:09,440 --> 00:00:14,560\n",
      "NVIDIA stopped taking hits, and the AI industry rethinking everything. And you might ask why?\n",
      "\n",
      "4\n",
      "00:00:14,560 --> 00:00:18,480\n",
      "This is because it delivers top-tier AI performance for a fraction\n",
      "\n",
      "5\n",
      "00:00:18,480 --> 00:00:21,280\n",
      "of the usual cost, and it was built in the last two months.\n",
      "\n",
      "6\n",
      "00:00:21,280 --> 00:00:26,320\n",
      "If AI companies no longer need billion-dollar budgets to train their models, what happens next?\n",
      "\n",
      "7\n",
      "00:00:26,320 --> 00:00:27,360\n",
      "Let's break it down.\n",
      "\n",
      "8\n",
      "00:00:27,360 --> 00:00:32,959\n",
      "So, what is DeepSeek R1? In simple terms, it's an open-source AI model out of China\n",
      "\n",
      "9\n",
      "00:00:32,959 --> 00:00:37,279\n",
      "that's punching way above its weight. In fact, it's on par with OpenAI's $200-per-month\n",
      "\n",
      "10\n",
      "00:00:37,279 --> 00:00:42,400\n",
      "R1 model at a lot of things like coding, research, and even maths. And it's free.\n",
      "\n",
      "11\n",
      "00:00:42,400 --> 00:00:46,080\n",
      "You can even host it yourself if you don't trust them. But what makes it really special\n",
      "\n",
      "12\n",
      "00:00:46,080 --> 00:00:50,400\n",
      "is the fact that it was trained for just $5.5 million, roughly $6 million.\n",
      "\n",
      "13\n",
      "00:00:50,959 --> 00:00:55,680\n",
      "In the AI world right now, that's like getting a brand new Tesla for the price of a used Honda\n",
      "\n",
      "14\n",
      "00:00:56,320 --> 00:01:01,599\n",
      "Civic. Compare that to OpenAI's GPT-4, which reportedly costs upwards of $200 million to train.\n",
      "\n",
      "15\n",
      "00:01:01,599 --> 00:01:03,919\n",
      "And you start to see why this is making waves.\n",
      "\n",
      "16\n",
      "00:01:03,919 --> 00:01:06,000\n",
      "But here's where everything gets crazier.\n",
      "\n",
      "17\n",
      "00:01:06,000 --> 00:01:09,199\n",
      "DeepSeek R1 wasn't trained on NVIDIA's most powerful GPUs.\n",
      "\n",
      "18\n",
      "00:01:09,199 --> 00:01:14,320\n",
      "Thanks to US sanctions, China doesn't have access to NVIDIA's cutting-edge AI chips like\n",
      "\n",
      "19\n",
      "00:01:14,320 --> 00:01:19,919\n",
      "the H100. Instead, they have to make do with the H800, which are essentially NVIDIA's\n",
      "\n",
      "20\n",
      "00:01:19,919 --> 00:01:24,480\n",
      "nerfed versions for the Chinese market. And yet, despite their hardware disadvantage,\n",
      "\n",
      "21\n",
      "00:01:24,480 --> 00:01:30,239\n",
      "DeepSeek R1 performs on par with OpenAI's O1 model, and even clawed 3.5% from Anthropic.\n",
      "\n",
      "22\n",
      "00:01:30,239 --> 00:01:32,239\n",
      "Now, let's talk about why this matters.\n",
      "\n",
      "23\n",
      "00:01:32,239 --> 00:01:37,120\n",
      "First off, NVIDIA's stock took a hit. You may be thinking why? Because if AI companies\n",
      "\n",
      "24\n",
      "00:01:37,120 --> 00:01:42,239\n",
      "can train high-performing models using cheaper and less powerful hardware, it is assumed that\n",
      "\n",
      "25\n",
      "00:01:42,239 --> 00:01:47,120\n",
      "the demand for NVIDIA's top-tier AI chips could shrink. And considering NVIDIA's valuation has\n",
      "\n",
      "26\n",
      "00:01:47,120 --> 00:01:52,639\n",
      "been largely propped by the AI boom, anything that threatens that demand sends investors into\n",
      "\n",
      "27\n",
      "00:01:52,639 --> 00:01:57,360\n",
      "a panic. And it's not just NVIDIA feeling the heat. If AI models can now be trained\n",
      "\n",
      "28\n",
      "00:01:57,360 --> 00:02:02,720\n",
      "cheaper and more efficiently, companies like OpenAI, Google, and Microsoft have to rethink\n",
      "\n",
      "29\n",
      "00:02:02,720 --> 00:02:07,680\n",
      "their strategies. DeepSeek's open-source decision has already forced OpenAI into action. I mean,\n",
      "\n",
      "30\n",
      "00:02:07,680 --> 00:02:11,839\n",
      "Sam Altman announced that the O1 model, which was previously locked behind a paywall,\n",
      "\n",
      "31\n",
      "00:02:11,839 --> 00:02:16,800\n",
      "will now be free. Clearly, a reaction to DeepSeek making cutting-edge AI accessible to everyone.\n",
      "\n",
      "32\n",
      "00:02:16,800 --> 00:02:21,440\n",
      "OpenAI realizes that if people have to access a powerful model like DeepSeek R1,\n",
      "\n",
      "33\n",
      "00:02:21,440 --> 00:02:24,320\n",
      "they need to match that move or risk losing their dominance.\n",
      "\n",
      "34\n",
      "00:02:24,320 --> 00:02:28,960\n",
      "DeepSeek R1 is already proving to be a serious competitor in the AI space. On the App Store\n",
      "\n",
      "35\n",
      "00:02:28,960 --> 00:02:34,479\n",
      "right now, it has overtaken ChatGPT to become one of the most downloaded AI apps. This isn't\n",
      "\n",
      "36\n",
      "00:02:34,479 --> 00:02:39,039\n",
      "just a niche product for AI enthusiasts. Regular users are flocking to it, knowing that there is\n",
      "\n",
      "37\n",
      "00:02:39,039 --> 00:02:43,919\n",
      "a real demand for an open-source alternative to OpenAI's world-guarding approach.\n",
      "\n",
      "38\n",
      "00:02:43,919 --> 00:02:48,720\n",
      "But here's where things get really interesting. The AI arms race just entered a new phase,\n",
      "\n",
      "39\n",
      "00:02:48,720 --> 00:02:53,679\n",
      "and one where open-source models are becoming serious contenders. And that means AI development\n",
      "\n",
      "40\n",
      "00:02:53,679 --> 00:02:59,679\n",
      "could start moving at an even faster pace. Think about it. When OpenAI keeps ChatGPT-4 under lock\n",
      "\n",
      "41\n",
      "00:02:59,679 --> 00:03:04,559\n",
      "and key, only a handful of companies get to improve it. But when a model like DeepSeek R1\n",
      "\n",
      "42\n",
      "00:03:04,559 --> 00:03:08,960\n",
      "is released as open-source, thousands of developers around the world can refine it,\n",
      "\n",
      "43\n",
      "00:03:08,960 --> 00:03:13,520\n",
      "optimize it, and push the technology forward in ways a single company never could.\n",
      "\n",
      "44\n",
      "00:03:13,520 --> 00:03:16,880\n",
      "This could be the beginning of the end for closed-source AI dominance.\n",
      "\n",
      "45\n",
      "00:03:16,880 --> 00:03:20,800\n",
      "I know there's glamour from Meta, but it's good to see that there are more options out there\n",
      "\n",
      "46\n",
      "00:03:20,800 --> 00:03:25,839\n",
      "doing the same thing. One of the biggest advantages of open-source AI models like DeepSeek R1\n",
      "\n",
      "47\n",
      "00:03:25,839 --> 00:03:31,360\n",
      "is accessibility. Unlike proprietary models such as OpenAI's GPT-4, which requires massive funding\n",
      "\n",
      "48\n",
      "00:03:31,360 --> 00:03:36,000\n",
      "and closed-door collaborations, open-source models lower the barrier to entry. Take Meta's\n",
      "\n",
      "49\n",
      "00:03:36,000 --> 00:03:40,960\n",
      "glamour models for example. These pre-trained models allow developers, researchers, and startups\n",
      "\n",
      "50\n",
      "00:03:40,960 --> 00:03:45,360\n",
      "to build upon them rather than starting from scratch, which would be nearly impossible for\n",
      "\n",
      "51\n",
      "00:03:45,360 --> 00:03:50,960\n",
      "individuals due to their high computational and financial costs. Additionally, open-source\n",
      "\n",
      "52\n",
      "00:03:50,960 --> 00:03:55,600\n",
      "approach encourages transparency, allowing researchers to scrutinize the model, detect\n",
      "\n",
      "53\n",
      "00:03:55,600 --> 00:04:00,880\n",
      "biases, and improve the safety measures. With more eyes on the code and training methodologies,\n",
      "\n",
      "54\n",
      "00:04:00,880 --> 00:04:05,279\n",
      "AI development becomes more accountable and aligned with real-world applications.\n",
      "\n",
      "55\n",
      "00:04:05,279 --> 00:04:09,919\n",
      "This kind of collaborative progress is something that closed-source models can't replicate at\n",
      "\n",
      "56\n",
      "00:04:09,919 --> 00:04:14,080\n",
      "the same scale. While DeepSeek R1 is the model everyone is talking about right now,\n",
      "\n",
      "57\n",
      "00:04:14,160 --> 00:04:18,799\n",
      "it's actually DeepSeek V3 running underneath, and this is how it works. I mean, this is a simplified\n",
      "\n",
      "58\n",
      "00:04:18,799 --> 00:04:23,359\n",
      "version of how it works, but it's much more complex in real life. DeepSeek uses an approach\n",
      "\n",
      "59\n",
      "00:04:23,359 --> 00:04:28,160\n",
      "called mixture of experts. Instead of treating every query as a general problem, it delegates\n",
      "\n",
      "60\n",
      "00:04:28,160 --> 00:04:32,959\n",
      "tasks to specialized sub-models trained for specific functions. So when you ask DeepSeek\n",
      "\n",
      "61\n",
      "00:04:32,959 --> 00:04:37,519\n",
      "a question, it figures which experts should handle it and routes that question or request\n",
      "\n",
      "62\n",
      "00:04:37,519 --> 00:04:41,920\n",
      "accordingly. This means that if you're running an 8 billion parameter model, it doesn't need to\n",
      "\n",
      "63\n",
      "00:04:41,920 --> 00:04:46,559\n",
      "load all the 8 billion parameters into memory, just the portion relevant to your query. That\n",
      "\n",
      "64\n",
      "00:04:46,559 --> 00:04:51,920\n",
      "dramatically reduces the memory usage, speeds up the response time, and lowers the power consumption.\n",
      "\n",
      "65\n",
      "00:04:51,920 --> 00:04:56,720\n",
      "It's a smarter and more efficient way to handle AI processing. I've personally spent the past\n",
      "\n",
      "66\n",
      "00:04:56,720 --> 00:05:01,760\n",
      "three days extensively testing DeepSeek R1, and I have to say it's an absolute game-changer.\n",
      "\n",
      "67\n",
      "00:05:01,760 --> 00:05:06,720\n",
      "It showcases an advanced level of reasoning, handling complex logical problems with a\n",
      "\n",
      "68\n",
      "00:05:06,720 --> 00:05:11,119\n",
      "precision I've rarely seen in open-source AI. It has the ability to perform chain-of-thought\n",
      "\n",
      "69\n",
      "00:05:11,119 --> 00:05:15,679\n",
      "reasoning, so instead of spitting out an answer immediately, it works through the problem step\n",
      "\n",
      "70\n",
      "00:05:15,679 --> 00:05:20,160\n",
      "by step, evaluating and even correcting itself in real-time. You'll see the whole thought process\n",
      "\n",
      "71\n",
      "00:05:20,160 --> 00:05:25,279\n",
      "as it runs. If DeepSeek can achieve this level of reasoning with lower-cost training, it suggests\n",
      "\n",
      "72\n",
      "00:05:25,279 --> 00:05:30,079\n",
      "that AI models no longer need billion-dollar budgets. That completely shifts the AI race,\n",
      "\n",
      "73\n",
      "00:05:30,079 --> 00:05:34,720\n",
      "making high-end capabilities accessible to more players, and potentially threatening the current\n",
      "\n",
      "74\n",
      "00:05:34,720 --> 00:05:39,760\n",
      "AI giants who have relied on their deep pockets to maintain dominance. Some analysts are questioning\n",
      "\n",
      "75\n",
      "00:05:39,760 --> 00:05:45,279\n",
      "whether DeepSeek R1 was really trained on as few GPUs as claimed. Others speculate that China might\n",
      "\n",
      "76\n",
      "00:05:45,279 --> 00:05:50,640\n",
      "have secretly used more powerful hardware, or borrowed techniques from existing Western models.\n",
      "\n",
      "77\n",
      "00:05:50,640 --> 00:05:56,079\n",
      "I even saw screenshots of DeepSeek responding as ChatGPT on the web, and that allegedly means it\n",
      "\n",
      "78\n",
      "00:05:56,079 --> 00:06:00,720\n",
      "was trained on ChatGPT's data. I can't really verify how true that is, but even if there's\n",
      "\n",
      "79\n",
      "00:06:00,720 --> 00:06:05,760\n",
      "some truth to that, it doesn't change the bigger picture. AI is getting cheaper and more accessible.\n",
      "\n",
      "80\n",
      "00:06:05,760 --> 00:06:09,679\n",
      "By the way, if you're skeptical about data privacy and worried about DeepSeek\n",
      "\n",
      "81\n",
      "00:06:09,679 --> 00:06:14,160\n",
      "sending your queries to Chinese servers, let's be real, who says America isn't doing that right now?\n",
      "\n",
      "82\n",
      "00:06:14,160 --> 00:06:19,040\n",
      "Allegedly. The good news is, you don't have to rely on cloud-based services to use DeepSeek.\n",
      "\n",
      "83\n",
      "00:06:19,040 --> 00:06:23,200\n",
      "You can run it locally, I mean it's open source. For instance, I'm running the 8 billion parameter\n",
      "\n",
      "84\n",
      "00:06:23,200 --> 00:06:29,359\n",
      "model on my MacBook Pro, the M1 chip, and it's between 4.5 to 5GB. Meanwhile, we've got the 14\n",
      "\n",
      "85\n",
      "00:06:29,359 --> 00:06:34,239\n",
      "billion parameter model running on our in-house server downstairs, and that's about 9GB. That\n",
      "\n",
      "86\n",
      "00:06:34,239 --> 00:06:39,359\n",
      "means all processing happens on device, and no data is sent over the internet. If you want a more\n",
      "\n",
      "87\n",
      "00:06:39,359 --> 00:06:44,880\n",
      "user-friendly way to run this, tools like OpenWebUI or LMStudio provide a great interface for running\n",
      "\n",
      "88\n",
      "00:06:44,880 --> 00:06:50,559\n",
      "AI models locally. So no subscriptions, no data privacy concerns, just pure local AI power.\n",
      "\n",
      "89\n",
      "00:06:50,559 --> 00:06:54,559\n",
      "But here's something else to consider. If training AI becomes more accessible,\n",
      "\n",
      "90\n",
      "00:06:54,559 --> 00:06:59,679\n",
      "it might actually create even more demand for AI infrastructure. And the lower barrier to entry\n",
      "\n",
      "91\n",
      "00:06:59,679 --> 00:07:05,040\n",
      "means more startups, research labs, and even individuals could jump into the AI game. And\n",
      "\n",
      "92\n",
      "00:07:05,040 --> 00:07:10,239\n",
      "inference, which is the process of using AI models, still requires massive amounts of computing power,\n",
      "\n",
      "93\n",
      "00:07:10,239 --> 00:07:14,160\n",
      "and that's where Nvidia still has an edge. Ironically, this could end up benefiting\n",
      "\n",
      "94\n",
      "00:07:14,160 --> 00:07:18,239\n",
      "Nvidia in the long run. Who knows? The initial stock dip could be just a temporary\n",
      "\n",
      "95\n",
      "00:07:18,239 --> 00:07:22,799\n",
      "market overreaction before the demand for AI infrastructure explodes again,\n",
      "\n",
      "96\n",
      "00:07:22,799 --> 00:07:26,000\n",
      "because we now know it can be cheaper to run these companies, right?\n",
      "\n",
      "97\n",
      "00:07:26,000 --> 00:07:29,760\n",
      "So what do you think? Is this the beginning of open-source AI revolution,\n",
      "\n",
      "98\n",
      "00:07:29,760 --> 00:07:33,920\n",
      "or just another hype cycle? Drop your thoughts in the comments section below. And if you enjoyed\n",
      "\n",
      "99\n",
      "00:07:33,920 --> 00:07:38,640\n",
      "this breakdown, do not forget to subscribe for more insights on how AI is shaping the future.\n",
      "\n",
      "100\n",
      "00:07:38,640 --> 00:07:42,959\n",
      "Until next time, stay curious and stay ahead of the curve. Catch you in the next video.\n",
      "\n",
      "101\n",
      "00:07:44,480 --> 00:07:46,880\n",
      "Cuidate!\n"
     ]
    }
   ],
   "source": [
    "subtitle = transcribe(\n",
    "    video,\n",
    "    prompt=\"DeepSeek Explained: The AI Game-Changer You Need to Know!\",  # 영상의 맥락을 설명해 정확도를 높임\n",
    "    response_format=\"srt\",  # 자막 제작을 위해 타이밍 표시\n",
    ")\n",
    "print(subtitle)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T08:41:38.808615Z",
     "start_time": "2025-02-03T08:41:18.059900Z"
    }
   },
   "id": "42d33c81a3f2e95d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"subtitle.srt\", \"w\") as f:\n",
    "  f.write(subtitle)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T08:41:38.815828Z",
     "start_time": "2025-02-03T08:41:38.811821Z"
    }
   },
   "id": "7ac5a42e3423c881",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 온전한 문장으로 만들기 위한 방법(녹취록 만들때)\n",
    "plaintext = transcribe(\n",
    "    video,\n",
    "    prompt=\"어요., 네요., 까요?, 니다.\"  # 마침표 추가\n",
    ")\n",
    "print(plaintext)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd7a61ce282c9cce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
